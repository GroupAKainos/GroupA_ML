{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e727f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e16c7",
   "metadata": {},
   "source": [
    "## Dataset (Pavlick Formality Scores)\n",
    "**Download**\n",
    "\n",
    "To enable this notebook to run, please download the following .csv files  from [here](https://huggingface.co/datasets/osyvokon/pavlick-formality-scores/tree/main) and save them in ./data (Hint: click on the file size):\n",
    "- all.csv\n",
    "- test.csv\n",
    "- train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720c159",
   "metadata": {},
   "source": [
    "### All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81392e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...\n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.\n",
       "2  answers       -2.0                       Good Luck and don't give up!\n",
       "3  answers        0.2     Most SHC victims are found near a heat source.\n",
       "4  answers       -1.6                      Tanay: I did have an opinion."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all data from csv into pandas dataframe\n",
    "df_all = pd.read_csv('./data/all.csv')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bdd07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11274, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7f51b",
   "metadata": {},
   "source": [
    "## AI003 ticket:\n",
    "1. Remove null values\n",
    "2. Tokenization (sentences to words)\n",
    "3. Normalisation\n",
    "    - Stemming (Create a function to stem text with chosen stemmer)\n",
    "    - Lemmatisation\n",
    "4. Use some stopwords (carefully curate)\n",
    "5. Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459a655",
   "metadata": {},
   "source": [
    "### Check for data for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d65bbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values present: domain       0\n",
      "avg_score    0\n",
      "sentence     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_sum = df_all.isnull().sum()\n",
    " \n",
    "# printing the number of null values present\n",
    "print('Number of NaN values present: ' + str(null_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638262d",
   "metadata": {},
   "source": [
    "No null values to remove in the dataset :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f3b94",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af874c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>[Pimp, Successfully, complete, all, the, Snatc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>[it, a, Holiday, Inn, for, terroists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>[Good, Luck, and, do, give, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>[Most, SHC, victims, are, found, near, a, heat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>[Tanay, I, did, have, an, opinion]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [Pimp, Successfully, complete, all, the, Snatc...  \n",
       "1              [it, a, Holiday, Inn, for, terroists]  \n",
       "2                    [Good, Luck, and, do, give, up]  \n",
       "3  [Most, SHC, victims, are, found, near, a, heat...  \n",
       "4                 [Tanay, I, did, have, an, opinion]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [w for w in tokens if w.isalpha()]\n",
    "\n",
    "df_all['tokenized'] = df_all.apply(lambda x: tokenize(x['sentence']), axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ba8d0",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b8c772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>[Pimp, Successfully, complete, all, the, Snatc...</td>\n",
       "      <td>[pimp, success, complet, all, the, snatch, loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>[it, a, Holiday, Inn, for, terroists]</td>\n",
       "      <td>[it, a, holiday, inn, for, terroist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>[Good, Luck, and, do, give, up]</td>\n",
       "      <td>[good, luck, and, do, give, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>[Most, SHC, victims, are, found, near, a, heat...</td>\n",
       "      <td>[most, shc, victim, are, found, near, a, heat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>[Tanay, I, did, have, an, opinion]</td>\n",
       "      <td>[tanay, i, did, have, an, opinion]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Pimp, Successfully, complete, all, the, Snatc...   \n",
       "1              [it, a, Holiday, Inn, for, terroists]   \n",
       "2                    [Good, Luck, and, do, give, up]   \n",
       "3  [Most, SHC, victims, are, found, near, a, heat...   \n",
       "4                 [Tanay, I, did, have, an, opinion]   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [pimp, success, complet, all, the, snatch, loc...  \n",
       "1               [it, a, holiday, inn, for, terroist]  \n",
       "2                    [good, luck, and, do, give, up]  \n",
       "3  [most, shc, victim, are, found, near, a, heat,...  \n",
       "4                 [tanay, i, did, have, an, opinion]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem(token):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(w) for w in token]\n",
    "\n",
    "df_all['stemmed'] = df_all.apply(lambda x: stem(x['tokenized']), axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d9b36",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b9288",
   "metadata": {},
   "source": [
    "Lemmatisation is not used as pre-processing step, as information in the words that can be used to determine formality may be lost. For example, lemmatising 'better' to 'good' may reduce the effectiveness of a model to determine to formality as words can be transformed into more or less formal lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20519c97",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4602a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>[Pimp, Successfully, complete, all, the, Snatc...</td>\n",
       "      <td>[pimp, success, complet, all, the, snatch, loc...</td>\n",
       "      <td>[pimp, success, complet, snatch, locat, level]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>[it, a, Holiday, Inn, for, terroists]</td>\n",
       "      <td>[it, a, holiday, inn, for, terroist]</td>\n",
       "      <td>[holiday, inn, terroist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>[Good, Luck, and, do, give, up]</td>\n",
       "      <td>[good, luck, and, do, give, up]</td>\n",
       "      <td>[good, luck, give]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>[Most, SHC, victims, are, found, near, a, heat...</td>\n",
       "      <td>[most, shc, victim, are, found, near, a, heat,...</td>\n",
       "      <td>[shc, victim, found, near, heat, sourc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>[Tanay, I, did, have, an, opinion]</td>\n",
       "      <td>[tanay, i, did, have, an, opinion]</td>\n",
       "      <td>[tanay, opinion]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Pimp, Successfully, complete, all, the, Snatc...   \n",
       "1              [it, a, Holiday, Inn, for, terroists]   \n",
       "2                    [Good, Luck, and, do, give, up]   \n",
       "3  [Most, SHC, victims, are, found, near, a, heat...   \n",
       "4                 [Tanay, I, did, have, an, opinion]   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [pimp, success, complet, all, the, snatch, loc...   \n",
       "1               [it, a, holiday, inn, for, terroist]   \n",
       "2                    [good, luck, and, do, give, up]   \n",
       "3  [most, shc, victim, are, found, near, a, heat,...   \n",
       "4                 [tanay, i, did, have, an, opinion]   \n",
       "\n",
       "                                          stopped  \n",
       "0  [pimp, success, complet, snatch, locat, level]  \n",
       "1                        [holiday, inn, terroist]  \n",
       "2                              [good, luck, give]  \n",
       "3         [shc, victim, found, near, heat, sourc]  \n",
       "4                                [tanay, opinion]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopper(token):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopped_sentence = []\n",
    "    for w in token:\n",
    "        if w not in stop_words:\n",
    "            stopped_sentence.append(w)\n",
    "    return stopped_sentence\n",
    "\n",
    "df_all['stopped'] = df_all.apply(lambda x: stopper(x['stemmed']), axis=1)\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1069c",
   "metadata": {},
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88623d2",
   "metadata": {},
   "source": [
    "1. Count Vectorization (Bag of Words)\n",
    "2. TF-IDF Vectorization\n",
    "3. Hashing Vectorization\n",
    "\n",
    "\n",
    "We will now be using SKLearn's built in text feature extraction library that can tokenize, stem and remove stopwords via a single function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf80712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [domain, avg_score, sentence]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first lets drop the the columns we added for 'tokenized', 'stemmed' and 'stopped'\n",
    "df_all = df_all.drop(['tokenized', 'stemmed', 'stopped'], axis=1)\n",
    "df_all.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461567c",
   "metadata": {},
   "source": [
    "### 1. Count vectorizer (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d9680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>count_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>(0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>(0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>(0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>(0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>(0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                        count_vector  \n",
       "0    (0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...  \n",
       "1      (0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1  \n",
       "2       (0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1  \n",
       "3    (0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...  \n",
       "4     (0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "params1 = {'lowercase' : True, #Convert all characters to lowercase before tokenizing\n",
    "          'stop_words': 'english', #Use sklearn built in corpus for stop word removal\n",
    "          'max_df': 1.0, #maximum document frequency: we can ignore words which occur frequently\n",
    "          'min_df': 0.0, #minimum document frequency: we can ignore words which occur infrequently\n",
    "          'analyzer' : 'word', #tokenize to words\n",
    "          'ngram_range': (1,1),\n",
    "         }\n",
    "\n",
    "vectorizer1 = CountVectorizer(**params1) #initialise the vectorizer\n",
    "vectorizer1.fit(df_all['sentence'].tolist()) #fit vectorizer to entire corpus\n",
    "\n",
    "df_all['count_vector'] = df_all.apply(lambda x: vectorizer1.transform([x['sentence']]), axis=1)\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107aafa8",
   "metadata": {},
   "source": [
    "We can view the format of each sentence vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289f46a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17324 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['count_vector'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a621131",
   "metadata": {},
   "source": [
    "And the locations in which the words appear in the feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78e06de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5077, 6993, 9466], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['count_vector'].iloc[2].nonzero()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461907ca",
   "metadata": {},
   "source": [
    "And how this relates to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731e1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \n",
      " Good Luck and don't give up!\n",
      "Sentence transformed back from vector: \n",
      " ['don' 'good' 'luck']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer1.get_feature_names_out()\n",
    "\n",
    "print('Original sentence: \\n', df_all['sentence'].iloc[2])\n",
    "print('Sentence transformed back from vector: \\n', features[df_all['count_vector'].iloc[2].nonzero()[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2037144",
   "metadata": {},
   "source": [
    "This example shows that stemming and stopwords are working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd24c40",
   "metadata": {},
   "source": [
    "### 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31a40bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>count_vector</th>\n",
       "      <th>tf_idf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>(0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...</td>\n",
       "      <td>(0, 15103)\\t0.3894924264295973\\n  (0, 14441)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>(0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1</td>\n",
       "      <td>(0, 15577)\\t0.6225307646760007\\n  (0, 8217)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>(0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1</td>\n",
       "      <td>(0, 9466)\\t0.7093871675370852\\n  (0, 6993)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>(0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...</td>\n",
       "      <td>(0, 16600)\\t0.4528952035324624\\n  (0, 14546)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>(0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1</td>\n",
       "      <td>(0, 15403)\\t0.7086807170172069\\n  (0, 10993)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                        count_vector  \\\n",
       "0    (0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...   \n",
       "1      (0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1   \n",
       "2       (0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1   \n",
       "3    (0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...   \n",
       "4     (0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1   \n",
       "\n",
       "                                       tf_idf_vector  \n",
       "0    (0, 15103)\\t0.3894924264295973\\n  (0, 14441)...  \n",
       "1    (0, 15577)\\t0.6225307646760007\\n  (0, 8217)\\...  \n",
       "2    (0, 9466)\\t0.7093871675370852\\n  (0, 6993)\\t...  \n",
       "3    (0, 16600)\\t0.4528952035324624\\n  (0, 14546)...  \n",
       "4    (0, 15403)\\t0.7086807170172069\\n  (0, 10993)...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "params2 = {'lowercase' : True, #Convert all characters to lowercase before tokenizing\n",
    "          'stop_words': 'english', #Use sklearn built in corpus for stop word removal\n",
    "          'max_df': 1.0, #maximum document frequency: we can ignore words which occur frequently\n",
    "          'min_df': 0.0, #minimum document frequency: we can ignore words which occur infrequently\n",
    "          'analyzer' : 'word', #tokenize to words\n",
    "          'ngram_range': (1,1),\n",
    "         }\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(**params2) #initialise the vectorizer\n",
    "vectorizer2.fit(df_all['sentence'].tolist()) #fit vectorizer to entire corpus\n",
    "\n",
    "df_all['tf_idf_vector'] = df_all.apply(lambda x: vectorizer2.transform([x['sentence']]), axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90287d66",
   "metadata": {},
   "source": [
    "We can view the format of each sentence vector (which is the same as the count vectorizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc598e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17324 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['tf_idf_vector'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e97eb",
   "metadata": {},
   "source": [
    "And the locations in which the words appear in the feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e48cc2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9466, 6993, 5077], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['tf_idf_vector'].iloc[2].nonzero()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dcb3f",
   "metadata": {},
   "source": [
    "And how this relates to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a1c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \n",
      " Good Luck and don't give up!\n",
      "Sentence transformed back from vector: \n",
      " ['luck' 'good' 'don']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer2.get_feature_names_out()\n",
    "\n",
    "print('Original sentence: \\n', df_all['sentence'].iloc[2])\n",
    "print('Sentence transformed back from vector: \\n', features[df_all['tf_idf_vector'].iloc[2].nonzero()[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c713be",
   "metadata": {},
   "source": [
    "### 3. Hashing Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88985760",
   "metadata": {},
   "source": [
    "This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory.\n",
    "\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.\n",
    "\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    "- there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "\n",
    "- there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "\n",
    "- no IDF weighting as this would render the transformer stateful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119c8dcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>count_vector</th>\n",
       "      <th>tf_idf_vector</th>\n",
       "      <th>hashing_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>Pimp (10) Successfully complete all the Snatch...</td>\n",
       "      <td>(0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...</td>\n",
       "      <td>(0, 15103)\\t0.3894924264295973\\n  (0, 14441)...</td>\n",
       "      <td>(0, 929)\\t-0.3779644730092272\\n  (0, 3046)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>it's a Holiday Inn for terroists.</td>\n",
       "      <td>(0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1</td>\n",
       "      <td>(0, 15577)\\t0.6225307646760007\\n  (0, 8217)\\...</td>\n",
       "      <td>(0, 7725)\\t-0.5773502691896258\\n  (0, 13975)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answers</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Good Luck and don't give up!</td>\n",
       "      <td>(0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1</td>\n",
       "      <td>(0, 9466)\\t0.7093871675370852\\n  (0, 6993)\\t...</td>\n",
       "      <td>(0, 65)\\t0.5773502691896258\\n  (0, 2230)\\t0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answers</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Most SHC victims are found near a heat source.</td>\n",
       "      <td>(0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...</td>\n",
       "      <td>(0, 16600)\\t0.4528952035324624\\n  (0, 14546)...</td>\n",
       "      <td>(0, 1880)\\t-0.4472135954999579\\n  (0, 1898)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answers</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>Tanay: I did have an opinion.</td>\n",
       "      <td>(0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1</td>\n",
       "      <td>(0, 15403)\\t0.7086807170172069\\n  (0, 10993)...</td>\n",
       "      <td>(0, 261)\\t-0.5773502691896258\\n  (0, 363)\\t-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain  avg_score                                           sentence  \\\n",
       "0  answers       -1.4  Pimp (10) Successfully complete all the Snatch...   \n",
       "1  answers       -1.8                  it's a Holiday Inn for terroists.   \n",
       "2  answers       -2.0                       Good Luck and don't give up!   \n",
       "3  answers        0.2     Most SHC victims are found near a heat source.   \n",
       "4  answers       -1.6                      Tanay: I did have an opinion.   \n",
       "\n",
       "                                        count_vector  \\\n",
       "0    (0, 27)\\t1\\n  (0, 3540)\\t1\\n  (0, 9180)\\t1\\n...   \n",
       "1      (0, 7640)\\t1\\n  (0, 8217)\\t1\\n  (0, 15577)\\t1   \n",
       "2       (0, 5077)\\t1\\n  (0, 6993)\\t1\\n  (0, 9466)\\t1   \n",
       "3    (0, 7452)\\t1\\n  (0, 10541)\\t1\\n  (0, 14069)\\...   \n",
       "4     (0, 4756)\\t1\\n  (0, 10993)\\t1\\n  (0, 15403)\\t1   \n",
       "\n",
       "                                       tf_idf_vector  \\\n",
       "0    (0, 15103)\\t0.3894924264295973\\n  (0, 14441)...   \n",
       "1    (0, 15577)\\t0.6225307646760007\\n  (0, 8217)\\...   \n",
       "2    (0, 9466)\\t0.7093871675370852\\n  (0, 6993)\\t...   \n",
       "3    (0, 16600)\\t0.4528952035324624\\n  (0, 14546)...   \n",
       "4    (0, 15403)\\t0.7086807170172069\\n  (0, 10993)...   \n",
       "\n",
       "                                      hashing_vector  \n",
       "0    (0, 929)\\t-0.3779644730092272\\n  (0, 3046)\\t...  \n",
       "1    (0, 7725)\\t-0.5773502691896258\\n  (0, 13975)...  \n",
       "2    (0, 65)\\t0.5773502691896258\\n  (0, 2230)\\t0....  \n",
       "3    (0, 1880)\\t-0.4472135954999579\\n  (0, 1898)\\...  \n",
       "4    (0, 261)\\t-0.5773502691896258\\n  (0, 363)\\t-...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "params3 = {'n_features': 2**14,#The number of features (columns) in the output matrices.\n",
    "           #Small numbers of features are likely to cause hash collisions,\n",
    "           #but large numbers will cause larger coefficient dimensions in linear learners.\n",
    "           'lowercase' : True, #Convert all characters to lowercase before tokenizing\n",
    "           'stop_words': 'english', #Use sklearn built in corpus for stop word removal\n",
    "           'analyzer' : 'word', #tokenize to words\n",
    "           'ngram_range': (1,1),\n",
    "          }\n",
    "\n",
    "vectorizer3 = HashingVectorizer(**params3) #initialise the vectorizer\n",
    "vectorizer3.fit(df_all['sentence'].tolist()) #fit vectorizer to entire corpus\n",
    "\n",
    "df_all['hashing_vector'] = df_all.apply(lambda x: vectorizer3.transform([x['sentence']]), axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc9ec7",
   "metadata": {},
   "source": [
    "We can view the format of each sentence vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffa77755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x16384 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['hashing_vector'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489f849",
   "metadata": {},
   "source": [
    "By using the hashing vectorizer, we can reduce the size of the sparse format vectors by reducing the 'n_features' parameter. The smaller these vectors are, the more efficient training and inference will be. However, the downside of not being able to compute the inverse transform means that when evaluating the model, one will not be able to determine which language the model determines as being a good indicator for informal or formal sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
